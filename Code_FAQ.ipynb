{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66795d53-27ff-4bd7-8810-8e93b638b943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 07:01:37.984366: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-09-21 07:01:37.987768: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-09-21 07:01:38.026023: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-09-21 07:01:39.062895: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface_hub: 0.27.1\nsentence_transformers: 2.7.0\ntransformers: 4.41.2\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f3c7c7bd4846b5b89f7f80c991851f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1648f020a437451283f7ca3ae4b7d707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3fa8d0d73cc4c779fdea7301883313c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47178da65bae4cf9a5196752b85abbb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07cb24a4bc04c2c8c7aa5bdfeb86b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86cfee08f825464083a6e6efcb7ccbe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd33fe258c69455b956b308e1aea1404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c02e08c3ac4c24a305dee2c623c5fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9186815918d48219bfd9c2c2a1345f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f9e3303dcb4d81bb5b667fd496dc70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2337e4386e1d46758f5ba3af9c016d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded OK\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub, sentence_transformers, transformers\n",
    "print(\"huggingface_hub:\", huggingface_hub.__version__)\n",
    "print(\"sentence_transformers:\", sentence_transformers.__version__)\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# Should download or load from cache without import errors:\n",
    "_ = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(\"Model loaded OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e206e473-0112-42fa-8262-db9e0b367b55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "============================================================\n",
    "###  FAQ Assistant\n",
    "\n",
    "What you get:\n",
    "-    Load & clean FAQs (columns: prompt, response)\n",
    "-    Embedding retriever (Sentence-Transformers) + FAISS index\n",
    "-    Optional OpenAI RAG answer constrained to retrieved context\n",
    "-    Baseline TF-IDF retriever (for comparison)\n",
    "-    Evaluation: self-retrieval on prompts\n",
    "\n",
    "Setup:\n",
    "- Optional Set OPENAI_API_KEY in your environment/secret scopes, then USE_OPENAI = True\n",
    "\n",
    "============================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2402d691-8807-4370-b1b6-a9504de83658",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6852a525-6e95-43bb-88db-41394921b0dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from joblib import dump, load\n",
    "\n",
    "# Baseline TF-IDF (optional; useful for evaluation/comparison)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e6199d9-ab2a-42eb-8384-946abda727c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optional .env support for OPENAI_API_KEY\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv(override=False)\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00b24ef6-a1b7-4b2d-8112-c373c12533e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d45d67c5-460e-4892-bfea-b00c9b0e22cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Config â€” adjust as needed\n",
    "# -----------------------------\n",
    "CSV_PATH = \"faqs.csv\"      # e.g., \"Filepath  - faqs.csv\"\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384-dim, fast & accurate for FAQs\n",
    "TOP_K = 5\n",
    "USE_OPENAI = False                    # True if OPENAI_API_KEY is set\n",
    "\n",
    "print(\"Config ->\", {\n",
    "    \"CSV_PATH\": CSV_PATH,\n",
    "    \"EMBED_MODEL_NAME\": EMBED_MODEL_NAME,\n",
    "    \"TOP_K\": TOP_K,\n",
    "    \"USE_OPENAI\": USE_OPENAI\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "862f7678-740c-4465-9e70-483bf4423039",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dacefd0-d351-48b2-80cf-26877d2fe587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Text normalization\n",
    "# -----------------------------\n",
    "_URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "_WS_RE = re.compile(r\"\\s+\")\n",
    "def normalize_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.strip()\n",
    "    s = _URL_RE.sub(\" \", s)\n",
    "    s = s.lower()\n",
    "    s = _WS_RE.sub(\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f762b85-d258-46e3-a33c-685c5336b836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7762507-1c0b-416b-8e81-e10955e955de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Data structures\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class FAQItem:\n",
    "    prompt: str\n",
    "    response: str\n",
    "\n",
    "@dataclass\n",
    "class FAQCorpus:\n",
    "    items: List[FAQItem]\n",
    "    df: pd.DataFrame  # with columns: prompt, response, prompt_norm, response_norm\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingIndex:\n",
    "    index: faiss.Index\n",
    "    dim: int\n",
    "    id_to_row: List[int]\n",
    "    model_name: str\n",
    "\n",
    "@dataclass\n",
    "class TfidfIndex:\n",
    "    vectorizer: TfidfVectorizer\n",
    "    matrix: any\n",
    "    id_to_row: List[int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59cec427-aa50-42bd-83fc-356bccc7a62d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97c1f3b5-04c6-43c9-9d61-45e8efe59d7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Load & clean CSV\n",
    "# -----------------------------\n",
    "def load_faqs(csv_path: str) -> FAQCorpus:\n",
    "    df = pd.read_csv(\n",
    "        csv_path,\n",
    "        encoding=\"ISO-8859-1\",\n",
    "        engine=\"python\",\n",
    "        on_bad_lines=\"skip\",\n",
    "        escapechar=\"\\\\\"\n",
    "    )\n",
    "    expected = {\"prompt\", \"response\"}\n",
    "    lower_cols = set(df.columns.str.lower())\n",
    "    missing = expected - lower_cols\n",
    "    if missing:\n",
    "        raise ValueError(f\"CSV missing required columns: {missing}. Found: {list(df.columns)}\")\n",
    "    df.columns = [c.lower().strip() for c in df.columns]\n",
    "    df = df.dropna(subset=[\"prompt\", \"response\"])\n",
    "    df[\"prompt\"] = df[\"prompt\"].astype(str).str.strip()\n",
    "    df[\"response\"] = df[\"response\"].astype(str).str.strip()\n",
    "    # Remove duplicate prompts\n",
    "    df = df.drop_duplicates(subset=[\"prompt\"]).reset_index(drop=True)\n",
    "    # Normalized copies for indexing\n",
    "    df[\"prompt_norm\"] = df[\"prompt\"].apply(normalize_text)\n",
    "    df[\"response_norm\"] = df[\"response\"].apply(normalize_text)\n",
    "    items = [FAQItem(r[\"prompt\"], r[\"response\"]) for _, r in df.iterrows()]\n",
    "    return FAQCorpus(items=items, df=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b277c382-d6fe-48d7-98dc-6b84d5535996",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdacfc05-1c5a-406a-8e1f-64c00df22b68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Embeddings + FAISS\n",
    "# -----------------------------\n",
    "def load_embedder(model_name: str) -> SentenceTransformer:\n",
    "    return SentenceTransformer(model_name)\n",
    "\n",
    "def embed_texts(model: SentenceTransformer, texts: List[str], batch_size: int = 64, normalize: bool = True) -> np.ndarray:\n",
    "    embs = model.encode(texts, batch_size=batch_size, convert_to_numpy=True, show_progress_bar=True)\n",
    "    if normalize:\n",
    "        faiss.normalize_L2(embs)  # enables cosine via inner product\n",
    "    return embs\n",
    "\n",
    "def build_faiss_index(embeddings: np.ndarray, model_name: str) -> EmbeddingIndex:\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)  # IP on normalized vectors = cosine\n",
    "    index.add(embeddings)\n",
    "    id_to_row = list(range(embeddings.shape[0]))\n",
    "    return EmbeddingIndex(index=index, dim=dim, id_to_row=id_to_row, model_name=model_name)\n",
    "\n",
    "def search_faiss(eindex: EmbeddingIndex, embedder: SentenceTransformer, corpus: FAQCorpus, query: str, top_k: int = 5) -> List[Tuple[int, float]]:\n",
    "    q = normalize_text(query)\n",
    "    q_emb = embedder.encode([q], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    scores, idxs = eindex.index.search(q_emb, top_k)\n",
    "    scores = scores[0]; idxs = idxs[0]\n",
    "    mapped: List[Tuple[int, float]] = []\n",
    "    for i, score in zip(idxs.tolist(), scores.tolist()):\n",
    "        if i == -1:\n",
    "            continue\n",
    "        row_idx = corpus.df.index[eindex.id_to_row[i]]\n",
    "        mapped.append((row_idx, float(score)))\n",
    "    return mapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "720a261b-f9e1-43c5-9a3e-87ff674976e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b20daa58-37d3-4583-ad26-c2ad0b362dd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# TF-IDF baseline (for comparison)\n",
    "# -----------------------------\n",
    "def build_tfidf_index(corpus: FAQCorpus, ngram_max: int = 2, stop_words: str = \"english\") -> TfidfIndex:\n",
    "    vec = TfidfVectorizer(ngram_range=(1, ngram_max), min_df=1, stop_words=stop_words)\n",
    "    mat = vec.fit_transform(corpus.df[\"prompt_norm\"].tolist())\n",
    "    id_to_row = list(range(len(corpus.df)))\n",
    "    return TfidfIndex(vectorizer=vec, matrix=mat, id_to_row=id_to_row)\n",
    "\n",
    "def search_tfidf(tindex: TfidfIndex, corpus: FAQCorpus, query: str, top_k: int = 5) -> List[Tuple[int, float]]:\n",
    "    q = normalize_text(query)\n",
    "    q_vec = tindex.vectorizer.transform([q])\n",
    "    sims = cosine_similarity(q_vec, tindex.matrix)[0]\n",
    "    top_ids = np.argsort(-sims)[:top_k]\n",
    "    return [(corpus.df.index[tindex.id_to_row[i]], float(sims[i])) for i in top_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b62b993-d84d-4033-a79b-4082033bf19b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87e58a38-6b86-471c-96b5-a25ee735a3b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Answering\n",
    "# -----------------------------\n",
    "def format_context(corpus: FAQCorpus, hits: List[Tuple[int, float]]) -> List[Dict]:\n",
    "    ctx = []\n",
    "    for row_idx, score in hits:\n",
    "        r = corpus.df.iloc[row_idx]\n",
    "        ctx.append({\"prompt\": r[\"prompt\"], \"response\": r[\"response\"], \"score\": round(score, 4)})\n",
    "    return ctx\n",
    "\n",
    "def answer_direct(corpus: FAQCorpus, hits: List[Tuple[int, float]]) -> str:\n",
    "    if not hits:\n",
    "        return \"Sorry, I couldn't find an answer to that.\"\n",
    "    top_row, _ = hits[0]\n",
    "    return str(corpus.df.iloc[top_row][\"response\"])\n",
    "\n",
    "def answer_rag_openai(corpus: FAQCorpus, hits: List[Tuple[int, float]], user_query: str,\n",
    "                      model: str = \"gpt-4o\", api_key: Optional[str] = None, temperature: float = 0.2) -> str:\n",
    "    api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        return answer_direct(corpus, hits)\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=api_key)\n",
    "        context_blocks = format_context(corpus, hits)\n",
    "        system_prompt = (\n",
    "            \"You are a concise FAQ assistant. Answer ONLY from the provided context. \"\n",
    "            \"If the answer is not present, say you do not know.\"\n",
    "        )\n",
    "        user_msg = (\n",
    "            f\"User question:\\n{user_query}\\n\\n\"\n",
    "            \"Context (top matches):\\n\" + json.dumps(context_blocks, ensure_ascii=False, indent=2)\n",
    "        )\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_msg}\n",
    "            ]\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"[OpenAI error: {e}]\\n\" + answer_direct(corpus, hits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a25641fc-5e76-4576-a7a1-b92aee5d3a40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be3852eb-0670-42c4-b70c-47700c1f1611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Engine\n",
    "# -----------------------------\n",
    "class Engine:\n",
    "    def __init__(self, corpus: FAQCorpus,\n",
    "                 retriever: str = \"faiss\",\n",
    "                 embedder: Optional[SentenceTransformer] = None,\n",
    "                 eindex: Optional[EmbeddingIndex] = None,\n",
    "                 tindex: Optional[TfidfIndex] = None):\n",
    "        self.corpus = corpus\n",
    "        self.retriever = retriever\n",
    "        self.embedder = embedder\n",
    "        self.eindex = eindex\n",
    "        self.tindex = tindex\n",
    "        assert retriever in {\"faiss\", \"tfidf\"}, \"retriever must be 'faiss' or 'tfidf'\"\n",
    "\n",
    "    def _search(self, query: str, top_k: int) -> List[Tuple[int, float]]:\n",
    "        if self.retriever == \"faiss\":\n",
    "            return search_faiss(self.eindex, self.embedder, self.corpus, query, top_k=top_k)\n",
    "        else:\n",
    "            return search_tfidf(self.tindex, self.corpus, query, top_k=top_k)\n",
    "\n",
    "    def query(self, text: str, top_k: int = 5, use_openai: bool = False) -> Dict:\n",
    "        hits = self._search(text, top_k=top_k)\n",
    "        contexts = format_context(self.corpus, hits)\n",
    "        ans = answer_rag_openai(self.corpus, hits, text) if use_openai else answer_direct(self.corpus, hits)\n",
    "        return {\"query\": text, \"answer\": ans, \"matches\": contexts}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52e9fbc9-f79f-46f7-ba8b-b90dfe4fd03f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d75c5c8c-54fe-476f-b9f0-9512e405370b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Evaluation:\n",
    "# -----------------------------\n",
    "def evaluate_retriever(corpus: FAQCorpus,\n",
    "                       engine: Engine,\n",
    "                       top_k_list: List[int] = [1, 3, 5, 10]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Self-retrieval evaluation: for each prompt, query the retriever and check\n",
    "    whether the ground-truth row is in top-K. Also compute MRR@K (with max K).\n",
    "    \"\"\"\n",
    "    max_k = max(top_k_list)\n",
    "    n = len(corpus.df)\n",
    "    hits_per_k = {k: 0 for k in top_k_list}\n",
    "    rr_list: List[float] = []\n",
    "\n",
    "    for i in range(n):\n",
    "        q = corpus.df.iloc[i][\"prompt\"]\n",
    "        true_row_idx = corpus.df.index[i]\n",
    "        hits = engine._search(q, top_k=max_k)\n",
    "        rank = None\n",
    "        for pos, (row_idx, _) in enumerate(hits, start=1):\n",
    "            if row_idx == true_row_idx:\n",
    "                rank = pos\n",
    "                break\n",
    "        rr_list.append(1.0 / rank if rank is not None else 0.0)\n",
    "        for k in top_k_list:\n",
    "            if rank is not None and rank <= k:\n",
    "                hits_per_k[k] += 1\n",
    "\n",
    "    metrics = {f\"Hit@{k}\": round(hits_per_k[k] / n, 4) for k in top_k_list}\n",
    "    metrics[f\"MRR@{max_k}\"] = round(float(np.mean(rr_list)), 4)\n",
    "    metrics[\"queries\"] = n\n",
    "    return pd.DataFrame([metrics])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fddc42db-9455-41cc-ac28-044e2c98645f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Build everything & quick runs\n",
    "# -----------------------------\n",
    "corpus = load_faqs(CSV_PATH)\n",
    "print(f\"Loaded FAQs: {len(corpus.df)} rows\")\n",
    "\n",
    "# Build Embedding + FAISS\n",
    "embedder = load_embedder(EMBED_MODEL_NAME)\n",
    "embeddings = embed_texts(embedder, corpus.df[\"prompt_norm\"].tolist(), batch_size=64, normalize=True)\n",
    "eindex = build_faiss_index(embeddings, EMBED_MODEL_NAME)\n",
    "faiss_engine = Engine(corpus=corpus, retriever=\"faiss\", embedder=embedder, eindex=eindex)\n",
    "\n",
    "# Baseline TF-IDF (optional)\n",
    "tfidf_index = build_tfidf_index(corpus)\n",
    "tfidf_engine = Engine(corpus=corpus, retriever=\"tfidf\", tindex=tfidf_index)\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: ask()\n",
    "# -----------------------------\n",
    "def ask(q: str, top_k: int = TOP_K, use_openai: bool = USE_OPENAI, retriever: str = \"faiss\", show_matches: bool = True):\n",
    "    engine = faiss_engine if retriever == \"faiss\" else tfidf_engine\n",
    "    result = engine.query(q, top_k=top_k, use_openai=use_openai)\n",
    "    print(f\"Retriever: {retriever.upper()} | Question: {result['query']}\")\n",
    "    print(\"\\nAnswer:\\n\" + result[\"answer\"])\n",
    "    if show_matches:\n",
    "        print(\"\\nTop matches:\")\n",
    "        for i, m in enumerate(result[\"matches\"], start=1):\n",
    "            print(f\"{i}. score={m['score']:.4f} | Q: {m['prompt']}\")\n",
    "            print(f\"   A: {m['response'][:160]}{'...' if len(m['response'])>160 else ''}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Smoke tests (Can modify/remove)\n",
    "# -----------------------------\n",
    "print(\"-\"*80); ask(\"Do you provide any job assistance?\", retriever=\"faiss\", show_matches=False); print()\n",
    "print(\"-\"*80); ask(\"Does Power BI work on Mac?\", retriever=\"faiss\", show_matches=False); print()\n",
    "print(\"-\"*80); ask(\"Is there lifetime access?\", retriever=\"faiss\", show_matches=False); print()\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation (both retrievers)\n",
    "# -----------------------------\n",
    "print(\"\\nEvaluating FAISS retriever (this runs over the full dataset)...\")\n",
    "faiss_eval_df = evaluate_retriever(corpus, faiss_engine, top_k_list=[1,3,5,10])\n",
    "try:\n",
    "    display(faiss_eval_df)\n",
    "except Exception:\n",
    "    print(faiss_eval_df)\n",
    "\n",
    "print(\"\\nEvaluating TF-IDF retriever (baseline)...\")\n",
    "tfidf_eval_df = evaluate_retriever(corpus, tfidf_engine, top_k_list=[1,3,5,10])\n",
    "try:\n",
    "    display(tfidf_eval_df)\n",
    "except Exception:\n",
    "    print(tfidf_eval_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Code_FAQ",
   "widgets": {
    "faq_query": {
     "currentValue": "What is Microsoft",
     "nuid": "d183f0d7-b687-465a-af8c-8ad385d20e63",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Do you provide any job assistance?",
      "label": null,
      "name": "faq_query",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "Do you provide any job assistance?",
      "label": null,
      "name": "faq_query",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "retriever": {
     "currentValue": "faiss",
     "nuid": "ac03145c-be6a-49b3-82fc-17b4e9bbfecc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "faiss",
      "label": null,
      "name": "retriever",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "faiss",
        "tfidf"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "faiss",
      "label": null,
      "name": "retriever",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "faiss",
        "tfidf"
       ]
      }
     }
    },
    "use_openai": {
     "currentValue": "true",
     "nuid": "d7c223b1-1d39-4062-8d91-eda822121a97",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": null,
      "name": "use_openai",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "false",
        "true"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "false",
      "label": null,
      "name": "use_openai",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "false",
        "true"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}